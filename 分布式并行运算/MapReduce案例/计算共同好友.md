# 计算共同好友

## 准备

数据格式

```txt
A:B,C,D,F,E,O    代表 A 的单向好友有 BCDFEO
B:A,C,E,K        代表 B 的单向好友有 ACEK
C:F,A,D,I
```

输出

```txt
A-B    C,E    代表 A和B 的共同好友有 CE
```

## 分析

使用map将每一行数据拆分为

```txt
A:B,C   =>   {B : A} {C : A}  的形式
B:C     =>   {C : B}

输出
{C : (A B)}  {B : (A)}
```

这样就可以初步得到拥有 key 的人（value）的集合

通过这个集合可以得到集合中两个人（value）组合的共同好友（key），注意这里获取的好友只是单独的，由于是reduce所有不是合并的，那么就不能使用`Combine` 来中间处理

由于获取到的是如下的数据
```txt
B-C     A
B-D     A
B-F     A
B-G     A
A-E     B
A-F     B
B-G     C
B-H     C
B-K     C
E-F     C
```

所以还是需要map来处理，将值整合再一起，需要再定义一个mapreduce类，在map中直接输出即可

在reduce中需要对已有的value进行一个整个（字符串拼接）

## 代码

### 初步处理

#### Map

```java
public static class MapTask extends Mapper<LongWritable, Text, Text, Text>{  
    @Override  
    protected void map(LongWritable key, Text value, Context context)  
            throws IOException, InterruptedException {  
        String[] Info = value.toString().split(":");  
        String Name = Info[0];  
        String[] Friends = Info[1].split(",");  
  
        for (String Friend: Friends) {  
            context.write(new Text(Friend), new Text(Name));  
        }  
    }  
}
```

#### Reduce

```java
public static class ReduceTask extends Reducer<Text, Text, Text, Text>{  
    @Override  
    protected void reduce(Text key, Iterable<Text> values,  
                          Context context) throws IOException, InterruptedException {  
        // 排序  
        StringBuilder Friends = new StringBuilder();  
        for (Text value : values) {  
            Friends.append(value.toString()).append(",");  
        }  
        String str = Friends.toString();  
        String[] strs = str.substring(0, str.length() - 1).split(",");  
        Arrays.sort(strs);  
  
        for (int i = 0; i < strs.length - 1; ++i) {  
            for (int j = i + 1; j < strs.length; ++j) {  
                context.write(new Text(strs[i] + "-" + strs[j]), key);  
            }  
        }  
    }  
}
```

#### Main

```java
public static void main(String[] args) throws 
	IOException, ClassNotFoundException, InterruptedException {  
	//告诉系统运行的时候，使用root权限  
	System.setProperty("HADOOP_USER_NAME", "root");  
	Configuration conf = new Configuration();  
	conf.set("fs.defaultFS", "hdfs://master:9000");  
	//1.MR由Job对象去提交任务  
	Job job = Job.getInstance(conf);  
	//2.告知job提交的信息  
	job.setMapperClass(MapTask.class);  
	job.setReducerClass(ReduceTask.class);  
	job.setJarByClass(WordCount.class);  
	//3.告知MR，输出类型 （只需要设置输出类型）  
	//map的输出  
	job.setMapOutputKeyClass(Text.class);  
	job.setMapOutputValueClass(Text.class);  
	//reduce的输出  
	job.setOutputKeyClass(Text.class);  
	job.setOutputValueClass(Text.class);  

	//4.输入输出文件的路径设置  
	String output = "/bigdata/input/MutualFriend/new";  
	//加一个判断  
	FileSystem fileSystem = FileSystem.get(conf);  
	if(fileSystem.exists(new Path(output))) {  
		fileSystem.delete(new Path(output),true);  
	}  

	FileInputFormat.addInputPath(job, new Path("/bigdata/input/MutualFriend/friend.txt"));  
	FileOutputFormat.setOutputPath(job, new Path(output));  

	//为了测试  
	boolean b = job.waitForCompletion(true);  
	System.out.println(b ? "T" : "F");  
}  
```

### 二次处理

#### Map

```java
public static class MapTask extends Mapper<LongWritable, Text, Text, Text>{  
    @Override  
    protected void map(LongWritable key, Text value, Context context)  
            throws IOException, InterruptedException {  
        String[] Info = value.toString().split("\t");  
        String Key = Info[0];  
        context.write(new Text(Key), new Text(Info[1]));  
    }  
}
```


### Reduce

```java
public static class ReduceTask extends Reducer<Text,Text, Text,Text>{  
    @Override  
    protected void reduce(Text key, Iterable<Text> values,  
                          Context context) throws IOException, InterruptedException {  
        StringBuilder Friends = new StringBuilder();  
        for (Text friend: values) {  
            Friends.append(friend.toString()).append(",");  
        }  
        String str = Friends.toString();  
        //写出去  
        context.write(key, new Text(str.substring(0, str.length()-1)));  
    }  
}
```

#### Main

```java
public static void main(String[] args) throws   
	IOException, ClassNotFoundException, InterruptedException {  
    //告诉系统运行的时候，使用root权限  
    System.setProperty("HADOOP_USER_NAME", "root");  
    Configuration conf = new Configuration();  
    conf.set("fs.defaultFS", "hdfs://master:9000");  
    //1.MR由Job对象去提交任务  
    Job job = Job.getInstance(conf);  
    //2.告知job提交的信息  
    job.setMapperClass(MapTask.class);  
  
    job.setReducerClass(ReduceTask.class);  
    job.setJarByClass(WordCount.class);  
    //3.告知MR，输出类型 （只需要设置输出类型）  
    //map的输出  
    job.setMapOutputKeyClass(Text.class);  
    job.setMapOutputValueClass(Text.class);  
    //reduce的输出  
    job.setOutputKeyClass(Text.class);  
    job.setOutputValueClass(Text.class);  
  
    //4.输入输出文件的路径设置  
    String output = "/bigdata/output/MutualFriend/";  
    //加一个判断  
    FileSystem fileSystem = FileSystem.get(conf);  
    if(fileSystem.exists(new Path(output))) {  
        fileSystem.delete(new Path(output),true);  
    }  
  
    FileInputFormat.addInputPath(job, new Path("/bigdata/input/MutualFriend/new/"));  
    FileOutputFormat.setOutputPath(job, new Path(output));  
  
    //为了测试  
    boolean b = job.waitForCompletion(true);  
    System.out.println(b ? "T" : "F");  
}
```

需要注意的是初次处理的输出路径需要作为二次处理的出入路径

## 输出

### 初次处理

![](http://www.droliz.cn//markdown_img/Pasted%20image%2020220906100720.png)

### 二次处理

![](http://www.droliz.cn//markdown_img/Pasted%20image%2020220906100745.png)