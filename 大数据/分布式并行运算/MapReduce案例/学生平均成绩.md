# 学生平均成绩

## 准备

学生不同科目在不同文件夹，统计平均成绩，输出到新的文件中

成绩格式

```txt
张三   89
李四   90
```

## 代码

```java
import java.io.IOException;  
import java.util.StringTokenizer;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
public class Score {  
  
    public static class MapTask extends Mapper<LongWritable,Text,Text,IntWritable>{  
        @Override  
        protected void map(LongWritable key, Text value, Context context)  
                throws IOException, InterruptedException {  
            // TODO Auto-generated method stub  
            String lines = value.toString();  
            StringTokenizer st = new StringTokenizer(lines, "\n");  
            while (st.hasMoreElements()) {  
                StringTokenizer token = new StringTokenizer(st.nextToken(), " ");  
                String stName = token.nextToken();  
                int stScore = new Integer(token.nextToken());  
                context.write(new Text(stName), new IntWritable(stScore));  
            }  
        }  
    }  
  
    public static class ReduceTask extends Reducer<Text,IntWritable, Text,IntWritable>{  
        @Override  
        protected void reduce(Text key, Iterable<IntWritable> values,  
                              Context context) throws IOException, InterruptedException {  
            int sum = 0;  
            int count = 0;  
            for(IntWritable value : values) {  
                sum += value.get();  
                count++;  
            }  
            //写出去  
            if (count != 0) {  
                context.write(key, new IntWritable(sum / count));  
            }  
        }  
    }  
  
    public static void main(String[] args) 
	    throws IOException, InterruptedException, ClassNotFoundException {  
        System.setProperty("HADOOP_USER_NAME", "root");  
        Configuration conf = new Configuration();  
        conf.set("fs.defaultFS", "hdfs://master:9000");  
        //1.MR由Job对象去提交任务  
        Job job = Job.getInstance(conf);  
        //2.告知job提交的信息  
        job.setMapperClass(MapTask.class);  
        job.setReducerClass(ReduceTask.class);  
        job.setJarByClass(Score.class);  
        //3.告知MR，输出类型 （只需要设置输出类型）  
        //map的输出  
        job.setMapOutputKeyClass(Text.class);  
        job.setMapOutputValueClass(IntWritable.class);  
        //reduce的输出  
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(IntWritable.class);  
  
        //4.输入输出文件的路径设置  
        String output = "/bigdata/output/avgScore";  
        //加一个判断  
        FileSystem fileSystem = FileSystem.get(conf);  
        if(fileSystem.exists(new Path(output))) {  
            fileSystem.delete(new Path(output),true);  
        }  
  
        FileInputFormat.addInputPath(job, new Path("/bigdata/input/score/"));  
        FileOutputFormat.setOutputPath(job, new Path(output));  
  
        //为了测试  
        boolean b = job.waitForCompletion(true);  
        System.out.println(b ? "SUCCESS！！" : "FAIL");  
  
    }  
}
```


需要输出日志，则将`/opt/hadoop-2.9.2/etc/hadoop/log4j.properties`文件放到项目的`/src/main/java/resources`目录下，再配置`pom.xml`文件即可（记得配置pom.xml）

```xml
<?xml version="1.0" encoding="UTF-8"?>  
<project xmlns="http://maven.apache.org/POM/4.0.0"  
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">  
    <modelVersion>4.0.0</modelVersion>  
  
    <groupId>org.example</groupId>  
    <artifactId>MR_02</artifactId>  
    <version>1.0-SNAPSHOT</version>  
  
    <properties>  
        <maven.compiler.source>8</maven.compiler.source>  
        <maven.compiler.target>8</maven.compiler.target>  
    </properties>  
  
    <dependencies>  
    
        <dependency>  
            <groupId>junit</groupId>  
            <artifactId>junit</artifactId>  
            <version>4.11</version>  
            <scope>test</scope>  
        </dependency>  
  
        <dependency>  
            <groupId>org.apache.logging.log4j</groupId>  
            <artifactId>log4j-core</artifactId>  
            <version>2.6.2</version>  
        </dependency>  
  
        <dependency>  
            <groupId>org.apache.hadoop</groupId>  
            <artifactId>hadoop-hdfs</artifactId>  
            <version>2.9.2</version>  
        </dependency>  
        
        <dependency>  
            <groupId>org.apache.hadoop</groupId>  
            <artifactId>hadoop-common</artifactId>  
            <version>2.9.2</version>  
        </dependency>  
        
        <dependency>  
            <groupId>org.apache.hadoop</groupId>  
            <artifactId>hadoop-client</artifactId>  
            <version>2.9.2</version>  
        </dependency>  
        
    </dependencies>  
</project>
```