# 单词计数

scala编写使用spark框架

```scala
package com.spark.core.wordcount  
  
import org.apache.spark.{SparkConf, SparkContext}  
  
object WordCount {  
  def main(args: Array[String]): Unit = {  
    // 1、建立和 spark 框架的连接   类似 jdbc    val sparkConf = new SparkConf().setMaster("local").setAppName("WordCount")  
    val sc = new SparkContext(sparkConf)  
    // 2、执行业务操作  
    // 2.1、读取文件  
    val lines = sc.textFile("data/WordCount/")   // : RDD[String]  
    // 2.2、拆分    扁平化操作  
//    val words = lines.flatMap(_.split(" "))   // RDD[String]  
//    // 2.3、  
//    val wordGroup = words.groupBy(word => word)   // 分组   : RDD[String, iterable[String]]
//    val wordToCount = wordGroup map {   // 模式匹配  
//      case (word, list) => (word, list.size)  
//    }  
//    val array = wordToCount collect()   // 采集数据  
//    array.foreach(println)  
  
    lines.flatMap(_.split(" "))
      .groupBy(word => word)  
      .map {  
      case (word, list) if word != "" && list.nonEmpty => (word, list.size)  
    }.foreach(println)  
    // 3、关闭连接  
    sc.stop()  
  }  
}
```

上述代码中聚合单词个数时，仅仅只是获取了list的属性，并不能完全体现聚合

```scala
package com.spark.core.wordcount  
  
import org.apache.spark.{SparkConf, SparkContext}  
  
object WordCount {  
  def main(args: Array[String]): Unit = {  
    // 1、建立和 spark 框架的连接   类似 jdbc    
    val sparkConf = new SparkConf().setMaster("local").setAppName("WordCount")  
    val sc = new SparkContext(sparkConf)  
  
    val lines = sc.textFile("data/WordCount/")   // : RDD[String]  
    // 扁平化操作  
    val words = lines.flatMap(_.split(" "))   // RDD[String]  
    // 从 word => word, 1    val wordToOne = words.map(  
      word => (word, 1)  
    )  
    // 分组    word: (1, 1, 1)    val wordGroup = wordToOne.groupBy(  
      t => t._1   // word  
    )  
    // 计数  
    val wordToCount = wordGroup.map {  
      case (_, list) =>  
        list.reduce(  
          (left, right) => (left._1, left._2 + right._2)  
        )  
    }  
    val array: Array[(String, Int)] = wordToCount.collect()  
    array.foreach(println)  
  
    // 3、关闭连接  
    sc.stop()  
  }  
}
```

在spark中可以将分组和聚合使用`reduceByKey()`方法实现

```scala
package com.spark.core.wordcount  
  
import org.apache.spark.{SparkConf, SparkContext}  
  
object WordCount {  
  def main(args: Array[String]): Unit = {  
    // 1、建立和 spark 框架的连接   类似 jdbc    val sparkConf = new SparkConf().setMaster("local").setAppName("WordCount")  
    val sc = new SparkContext(sparkConf)  
  
    val lines = sc.textFile("data/WordCount/")   // : RDD[String]  
    // 扁平化操作  
    val words = lines.flatMap(_.split(" "))   // RDD[String]  
    // 从 word => word, 1    val wordToOne = words.map(  
      word => (word, 1)  
    )  
    // 对于相同的 key 执行操作  
    val wordToCount = wordToOne.reduceByKey(_ + _)  
  
    val array: Array[(String, Int)] = wordToCount.collect()  
    array.foreach(println)  
  
    // 3、关闭连接  
    sc.stop()  
  }  
}
```