
## DataFrame

以列（列名，列类型，列值）的形式构成的[分布式](https://so.csdn.net/so/search?q=%E5%88%86%E5%B8%83%E5%BC%8F&spm=1001.2101.3001.7020)的数据集。

在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，是一种特殊的RDD，是一个分布式的表，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。

![](http://www.droliz.cn/markdown_img/Pasted%20image%2020221002223212.png)


DataFrame底层是转换为RDD来运行

## DataSet

DataSet是在Spark1.6中添加的新的接口；

与RDD相比，保存了更多的描述信息，概念上等同于关系型数据库中的二维表；

与DataFrame相比，保存了类型信息，是强类型的，提供了编译时类型检查；

DataSet是DataFrame功能的一个拓展

**DataFrame表示 为DataSet[Row]**



![](http://www.droliz.cn/markdown_img/Pasted%20image%2020221002231350.png)

## IDEA

idea中想要使用`spark-sql`就需要导入依赖

```xml
<dependency>  
    <groupId>org.apache.spark</groupId>  
    <artifactId>spark-sql_2.11</artifactId>  
    <version>2.4.5</version>  
</dependency>
```

两种连接方式

```scala
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("test")  
val spark = SparkSession  
  .builder()  
  .config(sparkConf)  
  .getOrCreate()
```

```scala
val spark = SparkSession  
  .builder()  
  .master("local[*]")  
  .appName("test")  
  .getOrCreate()  
```

```scala
spark.close()  // 关闭连接
```

想要使用类型转换，那么需要导入隐式类型转换

```scala
import spark.implicits._   // spark是 SparkSession 对象
```

例如

```scala
import spark.implicits._  

val list = List(Person("zs", 18), Person("ls", 20))  
val df = list.toDF()    // 需要导入隐式类型转换
val ds = list.toDS()  

df.show  
ds.show
  
case class Person(name: String, age: Long)
```